# ============================
# üì¶ LIBRER√çAS Y DESCARGAS
# ============================
import nltk
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('wordnet')

from nltk.corpus import brown, stopwords, wordnet
from nltk.stem import PorterStemmer, WordNetLemmatizer
from collections import Counter
import pandas as pd


# ============================
# A. üßπ ELIMINACI√ìN DE RUIDO
# ============================

# Cargar el texto del archivo 'cg73'
raw_text = brown.words(fileids='cg73')

# Eliminar puntuaci√≥n y caracteres no alfab√©ticos
text_clean = [word for word in raw_text if word.isalpha()]


# ============================
# B. ‚úÇÔ∏è TOKENIZACI√ìN
# ============================

# Ya est√° tokenizado, solo seguimos con la lista limpia
tokens = text_clean


# ============================
# C. üîÑ NORMALIZACI√ìN
# ============================

# Pasar todo a min√∫sculas
tokens_norm = [word.lower() for word in tokens]


# ============================
# D. ‚ùå ELIMINACI√ìN DE STOPWORDS
# ============================

stop_words = set(stopwords.words('english'))
tokens_no_stop = [word for word in tokens_norm if word not in stop_words]


# ============================
# E. üìä TOP 50 PALABRAS FRECUENTES
# ============================

freq_50 = Counter(tokens_no_stop).most_common(50)
print("\nE. 50 palabras m√°s frecuentes (sin stopwords):")
print(freq_50)


# ============================
# F. üå± STEMMING
# ============================

stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(word) for word in tokens_no_stop]
freq_stemmed_50 = Counter(stemmed_tokens).most_common(50)
print("\nF. 50 palabras m√°s frecuentes (Stemming):")
print(freq_stemmed_50)


# ============================
# G. üåø LEMATIZACI√ìN
# ============================

lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens_no_stop]
freq_lemmatized_50 = Counter(lemmatized_tokens).most_common(50)
print("\nG. 50 palabras m√°s frecuentes (Lematizaci√≥n):")
print(freq_lemmatized_50)


# ============================
# H. üìå LEMATIZACI√ìN CON PoS
# ============================

# Funci√≥n para mapear etiquetas PoS
def get_wordnet_pos(tag):
    if tag.startswith('V'):
        return wordnet.VERB
    return wordnet.NOUN  # Por defecto

# Etiquetado PoS
pos_tags = nltk.pos_tag(tokens_no_stop)

# Lematizaci√≥n considerando PoS
lemmatized_pos = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]
freq_lemmatized_pos_50 = Counter(lemmatized_pos).most_common(50)
print("\nH. 50 palabras m√°s frecuentes (Lematizaci√≥n con PoS):")
print(freq_lemmatized_pos_50)


# ============================
# I. üìã TABLA DE PRIMEROS 30 TOKENS
# ============================

data = {
    "Token": tokens_no_stop[:30],
    "Stemming": [stemmer.stem(w) for w in tokens_no_stop[:30]],
    "Lematizaci√≥n": [lemmatizer.lemmatize(w) for w in tokens_no_stop[:30]],
    "Lematizaci√≥n_Pos": [
        lemmatizer.lemmatize(w, get_wordnet_pos(tag))
        for (w, tag) in nltk.pos_tag(tokens_no_stop[:30])
    ]
}

df = pd.DataFrame(data)

print("\nI. Tabla de los primeros 30 tokens con transformaciones:")
print(df)